{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Stocks.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiStockEnv:\n",
    "    \"\"\"\n",
    "    A 3-stock trading environment.\n",
    "    State: vector of size 7 (n_stock * 2 + 1)\n",
    "        - # shares of stock 1 owned\n",
    "        - # shares of stock 2 owned\n",
    "        - # shares of stock 3 owned\n",
    "        - price of stock 1 (using daily close price)\n",
    "        - price of stock 2\n",
    "        - price of stock 3\n",
    "        - cash owned (can be used to purchase more stocks)\n",
    "    Action: categorical variable with 27 (3^3) possibilities\n",
    "        - for each stock, you can:\n",
    "        - 0 = sell\n",
    "        - 1 = hold\n",
    "        - 2 = buy\n",
    "    \"\"\"\n",
    "    def __init__(self, data, initial_investment=20000):\n",
    "        # data\n",
    "        self.stock_price_history = data\n",
    "        self.n_step, self.n_stock = self.stock_price_history.shape\n",
    "\n",
    "        # instance attributes\n",
    "        self.initial_investment = initial_investment\n",
    "        self.cur_step = None\n",
    "        self.stock_owned = None\n",
    "        self.stock_price = None\n",
    "        self.cash_in_hand = None\n",
    "\n",
    "        self.action_space = np.arange(3**self.n_stock)\n",
    "\n",
    "        # action permutations\n",
    "        # returns a nested list with elements like:\n",
    "        # [0,0,0]\n",
    "        # [0,0,1]\n",
    "        # [0,0,2]\n",
    "        # [0,1,0]\n",
    "        # [0,1,1]\n",
    "        # etc.\n",
    "        # 0 = sell\n",
    "        # 1 = hold\n",
    "        # 2 = buy\n",
    "        self.action_list = list(map(list, itertools.product([0, 1, 2], repeat=self.n_stock)))\n",
    "\n",
    "        # calculate size of state\n",
    "        self.state_dim = self.n_stock * 2 + 1\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.cur_step = 0\n",
    "        self.stock_owned = np.zeros(self.n_stock)\n",
    "        self.stock_price = self.stock_price_history[self.cur_step]\n",
    "        self.cash_in_hand = self.initial_investment\n",
    "        return self._get_obs()\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        assert action in self.action_space\n",
    "\n",
    "        # get current value before performing the action\n",
    "        prev_val = self._get_val()\n",
    "\n",
    "        # update price, i.e. go to the next day\n",
    "        self.cur_step += 1\n",
    "        self.stock_price = self.stock_price_history[self.cur_step]\n",
    "\n",
    "        # perform the trade\n",
    "        self._trade(action)\n",
    "\n",
    "        # get the new value after taking the action\n",
    "        cur_val = self._get_val()\n",
    "\n",
    "        # reward is the increase in porfolio value\n",
    "        reward = cur_val - prev_val\n",
    "\n",
    "        # done if we have run out of data\n",
    "        done = self.cur_step == self.n_step - 1\n",
    "\n",
    "        # store the current value of the portfolio here\n",
    "        info = {'cur_val': cur_val}\n",
    "\n",
    "        # conform to the Gym API\n",
    "        return self._get_obs(), reward, done, info\n",
    "\n",
    "\n",
    "    def _get_obs(self):\n",
    "        obs = np.empty(self.state_dim)\n",
    "        obs[:self.n_stock] = self.stock_owned\n",
    "        obs[self.n_stock:2*self.n_stock] = self.stock_price\n",
    "        obs[-1] = self.cash_in_hand\n",
    "        return obs\n",
    "    \n",
    "    def _get_val(self):\n",
    "        return self.stock_owned.dot(self.stock_price) + self.cash_in_hand\n",
    "\n",
    "    def _trade(self, action):\n",
    "        # index the action we want to perform\n",
    "        # 0 = sell\n",
    "        # 1 = hold\n",
    "        # 2 = buy\n",
    "        # e.g. [2,1,0] means:\n",
    "        # buy first stock\n",
    "        # hold second stock\n",
    "        # sell third stock\n",
    "        action_vec = self.action_list[action]\n",
    "\n",
    "        # determine which stocks to buy or sell\n",
    "        sell_index = [] # stores index of stocks we want to sell\n",
    "        buy_index = [] # stores index of stocks we want to buy\n",
    "        for i, a in enumerate(action_vec):\n",
    "            if a == 0:\n",
    "                sell_index.append(i)\n",
    "            elif a == 2:\n",
    "                buy_index.append(i)\n",
    "\n",
    "        # sell any stocks we want to sell\n",
    "        # then buy any stocks we want to buy\n",
    "        if sell_index:\n",
    "            # NOTE: to simplify the problem, when we sell, we will sell ALL shares of that stock\n",
    "            for i in sell_index:\n",
    "                self.cash_in_hand += self.stock_price[i] * self.stock_owned[i]\n",
    "                self.stock_owned[i] = 0\n",
    "        if buy_index:\n",
    "            # NOTE: when buying, we will loop through each stock we want to buy,\n",
    "            #       and buy one share at a time until we run out of cash\n",
    "            can_buy = True\n",
    "            while can_buy:\n",
    "                for i in buy_index:\n",
    "                    if self.cash_in_hand > self.stock_price[i]:\n",
    "                        self.stock_owned[i] += 1 # buy one share\n",
    "                        self.cash_in_hand -= self.stock_price[i]\n",
    "                    else:\n",
    "                        can_buy = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list = list(map(list, itertools.product([0, 1, 2], repeat=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_timesteps, n_stocks = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = n_timesteps // 2\n",
    "n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:n_train].values\n",
    "test_data = data[n_train:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MultiStockEnv(train_data,initial_investment=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,W,b):\n",
    "    # make sure X is N x D\n",
    "    assert(len(X.shape) == 2)\n",
    "    return X.dot(W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X,Y,W,b,vW,vb,learning_rate=0.01, momentum=0.9):\n",
    "    assert(len(X.shape) == 2)\n",
    "    # the loss values are 2-D\n",
    "    # normally we would divide by N only\n",
    "    # but now we divide by N x K\n",
    "    num_values = np.prod(Y.shape)\n",
    "    # do one step of gradient descent\n",
    "    # we multiply by 2 to get the exact gradient\n",
    "    # (not adjusting the learning rate)\n",
    "    # i.e. d/dx (x^2) --> 2x\n",
    "    Yhat = predict(X,W,b)\n",
    "    gW = 2 * X.T.dot(Yhat - Y) / num_values\n",
    "    gb = 2 * (Yhat - Y).sum(axis=0) / num_values\n",
    "\n",
    "    # update momentum terms\n",
    "    vW = momentum * vW - learning_rate * gW\n",
    "    vb = momentum * vb - learning_rate * gb\n",
    "\n",
    "    # update params\n",
    "    W += vW\n",
    "    b += vb\n",
    "    return W,b,vW,vb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_policy(state,W,b,epsilon):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return np.random.choice(action_size)\n",
    "    else:\n",
    "        act_values = predict(state,W,b)\n",
    "        return np.argmax(act_values[0])  # returns action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaler(env):\n",
    "    # return scikit-learn scaler object to scale the states after fitting the scaler with some random actions\n",
    "    states = []\n",
    "    #play game 100 times with random actions to collect some states\n",
    "    for i in range(100):\n",
    "        for j in range(env.n_step):\n",
    "            action = np.random.choice(env.action_space)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            states.append(state)\n",
    "            if done:\n",
    "                env.reset()\n",
    "                break\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(states)\n",
    "    return scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95  # discount rate\n",
    "epsilon = 1.0  # exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "num_episodes = 2000\n",
    "initial_investment = 20000\n",
    "\n",
    "n_timesteps, n_stocks = data.shape\n",
    "n_train = n_timesteps // 2\n",
    "train_data = data[:n_train].values\n",
    "test_data = data[n_train:].values\n",
    "\n",
    "np.random.seed(1)\n",
    "env = MultiStockEnv(train_data,initial_investment=20000)\n",
    "#Train the scaler, the scaler only uses the training set\n",
    "state_size = env.state_dim\n",
    "action_size = len(env.action_space)\n",
    "\n",
    "#Initialize weights\n",
    "W = np.random.randn(state_size, action_size) / np.sqrt(state_size)\n",
    "b = np.zeros(action_size)\n",
    "scaler=get_scaler(env)\n",
    "\n",
    "#Initialize momentum terms\n",
    "vW = 0\n",
    "vb = 0\n",
    "\n",
    "# store the final value of the portfolio (end of episode)\n",
    "portfolio_value = []\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    done=False\n",
    "    state = env.reset()\n",
    "    state = scaler.transform([state])\n",
    "    while not done:\n",
    "        action = greedy_policy(state,W,b,epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = scaler.transform([next_state])\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + gamma * np.amax(predict(next_state,W,b), axis=1)\n",
    "        #Make prediction for all actions first\n",
    "        target_full = predict(state,W,b)\n",
    "        #Update target values for only the selected action \n",
    "        target_full[0, action] = target\n",
    "        #Update weights\n",
    "        W,b,vW,vb=sgd(X=state, Y=target_full,W=W,b=b,vW=vW,vb=vb)\n",
    "        \n",
    "        state=next_state\n",
    "        \n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "    val=info['cur_val']\n",
    "    print(f\"episode: {i + 1}/{num_episodes}, episode end value: {val:.2f}\")\n",
    "    portfolio_value.append(val)\n",
    "pickle.dump(scaler, open('scaler.pkl','wb'))\n",
    "np.savez(\"TraderWeights.npz\", W=W, b=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = pickle.load(open('scaler.pkl', 'rb'))\n",
    "npz = np.load(\"TraderWeights.npz\")\n",
    "W = npz['W']\n",
    "b = npz['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "#Test the performance of the agent with the test data\n",
    "epsilon = 0.01\n",
    "env = MultiStockEnv(test_data, initial_investment)\n",
    "testportfoliovalues=[]\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = scaler.transform([state])\n",
    "    done=False\n",
    "    while not done:\n",
    "        action = greedy_policy(state,W,b,epsilon=epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = scaler.transform([next_state])\n",
    "        state = next_state\n",
    "    value=info['cur_val']\n",
    "    print(f\"episode: {i + 1}/{num_episodes}, episode end value: {value:.2f}\")\n",
    "    testportfoliovalues.append(value) # append episode end portfolio value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "#How does a random policy do?\n",
    "epsilon = 1\n",
    "env = MultiStockEnv(test_data, initial_investment)\n",
    "randomportfoliovalues=[]\n",
    "for i in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    state = scaler.transform([state])\n",
    "    done=False\n",
    "    while not done:\n",
    "        action = greedy_policy(state,W,b,epsilon=epsilon)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        next_state = scaler.transform([next_state])\n",
    "        state = next_state\n",
    "    value=info['cur_val']\n",
    "    print(f\"episode: {i + 1}/{num_episodes}, episode end value: {value:.2f}\")\n",
    "    randomportfoliovalues.append(value) # append episode end portfolio value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"TraderPortfolios.npz\", TrainingPortfolio=portfolio_value, \n",
    "         TestPortfolio=testportfoliovalues,\n",
    "         RandomPortfolio=randomportfoliovalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load(\"TraderPortfolios.npz\")\n",
    "TrainingPortfolio=npz['TrainingPortfolio']\n",
    "RandomPortfolio=npz['RandomPortfolio']\n",
    "TestPortfolio=npz['TestPortfolio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(TrainingPortfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(TrainingPortfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(TestPortfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(TestPortfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(RandomPortfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(RandomPortfolio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PercentIncrease=(test_data[-1]-test_data[0])/test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(PercentIncrease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
